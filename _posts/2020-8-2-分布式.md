---
title: 分布式
auther: ahou
layout: post
description: 分布式相关知识
categories: general
---

## CAP理论
CAP理论是分布式系统的基础，分别是指一致性(Consistensy)，可用性(Availability)和分区容错性(Partition tolerance)，它们的含义是：  
- 一致性，分布式系统的多个节点数据保持一致  
- 可用性，系统能够在正常的时间内对请求做正确的响应  
- 分区容错性，当某个节点或分区发生故障时，系统仍然能够提供服务  

**CAP理论指出：分布式系统最多只能满足CAP中的两项**  
例如，当系统中节点A向节点B的网络发生故障，在保证分区容错的条件下，系统能够提供服务，但此时若访问A节点修改了数据x，由于网络故障无法及时的同步到B，那么如果要保证**一致性**，执行等待故障修复后B才能响应数据x的请求，违反可用性，如果要保证**可用性**，只能返回未同步的数据x，违反一致性。  

由于CAP理论提出的限制，分布式系统必须在CAP之中选择两个，但分区容错性P是分布式系统的根本，一般不能违背，所以分布式系统多实现了CP和AP  
对数据一致性要求严格的场合，选择CP，放弃一些情况下的可用性。  
对于一致性要求不那么高的场合，选择AP来保证可用性，但这时放弃的是强一致性(时刻一致)，可能是弱一致性或最终一致性。  


## 分布式事务

**参考文章：[article1](https://www.cnblogs.com/mayundalao/p/11798502.html)，[article2](https://baijiahao.baidu.com/s?id=1634759073848664238&wfr=spider&for=pc)**  

#### 介绍
分布式事务是指一个任务分解到多个节点上执行，如何保证它的提交与回滚操作，使它具有ACID的事务属性

#### 两阶段提交协议2PC
两阶段提交中，引入协调者来协调多个参与者的任务执行，来达到任务的事务属性  

- 准备阶段  
    协调者向参与者发送任务，参与者执行并返回执行结果，此时不执行commit操作
- 提交阶段  
    当准备阶段所有参与者都返回成功执行，协调者发送提交命令，参与者执行commit操作，否则回滚

**缺点：**  
- 同步阻塞问题，等待其他参与者响应过程中，所有参与者都处在同步阻塞中，无法执行其他操作
- 协调者向参与者发送commit命令时，如果由于网络问题导致有些参与者没有及时收到commit，会出现不一致问题
- 协调者的单点问题，协调者故障，整个执行都出现问题，特别是提交阶段，所有参与者都需等待

#### TCC
**参考文章：[TCC举例详细介绍](https://www.cnblogs.com/jajian/p/10014145.html)**  
补偿事务  
TCC分布式事务的执行分为三个阶段：
- **Try** 尝试执行事务，预留需要的资源  
    比如商品库存扣减中，添加一个预扣减字段，try阶段写该字段，如果写入成功，表明资源访问正常，有理由相信真正的扣减操作也是大概率成功的  
- **confirm** 当try阶段全部执行成功后，执行该操作，进行实际的业务操作  
    此时进行真正的业务操作，从商品库存中减去预扣减的值，将预扣减置零  
- **cancel** 当try阶段执行出错时，cancel之前的操作，将预留的资源释放掉  
    将预扣减的字段置零  

TCC分布式事务框架会保存活动日志，当出现出现机器宕机时，根据日志记录重新执行confirm
或者cancel操作，重试直到成功  

**优缺点：**
- 流程简单  
- confirm和cancel都可能失败  
- TCC位于应用层，需要配合实现业务处理，使用该方案就要实现Try阶段预留资源，confirm提交等逻辑，业务未必适合这种形式  

## 分布式系统一致性协议
### 多副本状态机
一个副本状态机从相同的初始状态开始，应用一个日志序列后，系统最终的状态是一致的，在多副本系统中，每个副本具有一个副本状态机，在通过paxos、raft这样的一致性协议系统中，提交相同的日志后，最终所有副本都有相同的状态  

### paxos
paxos是在异步通信下保证分布式系统实现强一致性的协议，可容忍系统不超过一半节点故障

paxos协议中一个决策过程分为两个阶段：  
- prepare
proposer确定一个提议id并向超过半数的accepter发起提议prepare，accepter收到prepare请求后，若id大于它之前收到的最大id，则返回其确认过的id最大的提议的value（没有则返回空），并且承诺不再响应id小于当前id的提议，当proposer收到半数以上的accepter确认后进入accept阶段  
- accept
proposer收到半数以上的accepter回复后，从中选择id最大的value做为提议的value，value全部为空则由proposer自己确定value，然后将该value做为提议的值发送给accepter执行accept，accepter接收到提议后再不违反承诺的情况下更新自己value的值，proposer收到大多多数accepter的成功响应后完成决策  

paxos协议可以保证决策的安全性，即只有一个提议会被接收，且被接受的提议不会再发生变化，但不保证决策过程的正常终止  
``` java
FLP定理是说在分布式异步网络环境下，协商确定一个值，没有算法即安全正确，又能正常终止。在paxos中很容易构造一个场景，让paxos协商值的过程变成活锁，永远无法终止。比如说： 节点A用10的提案编号给所有的节点发prepare请求，然后节点B用11的提案编号给所有的节点发prepare请求，然后A再去发accept指令的时候，会发现当前系统的时钟已经到11了，没有节点响应他的请求了，因此他只能用12的提案编号重新开始paxos的流程，发送prepare请求，然后节点B要发accept的时候，也看到系统的整体时钟已经到了12，没有节点响应他的请求了，他因此把提案编号增大成13，然后继续paxos的流程，一直这些循环往复，导致paxos无法进行下去。

paxos算法在任何情况下都保证了safety安全性，但是不保证liveness。liveness的问题一般都是用随机化的方法来规避，通过随机化让冲突发生的概率很低很低，即使发生了冲突，再随机化重试即可规避。
```
paxos保证决策的安全性主要通过半数以上决议和两阶段的过程，在第一阶段，proposer可学得当前系统所接受的最大提议的value，因为他需要半数以上的accepter响应，那么如果存在已被选定的提议，那么该提议的value一定会出现在accepter的响应中，且id最大，因为accepter在接收一个提议后不会响应更小的id  
那么proposer在accepter阶段发出的value一定时系统已经选定的id最大的提议的value  

### raft
raft是paxos的一种工程实现，其实现更容易理解和操作，同时他增加了leader选举，可对一个一个序列的提议进行决策，属于multi-paxos  

**[raft协议演示](http://thesecretlivesofdata.com/raft/)， [raft github]( https://raft.github.io/)**  

raft系统中的节点有三个状态，leader、follower、candidate，所有节点的状态在他们之间切换，其中  
- leader  负责处理client的日志写操作，并同步到follower中
- follower  负责接收leader的日志传递并响应，leader下线后可竞争成为leader
- candidate  该状态的节点处于竞争leader的选举阶段

raft将分布式系统的一致性问题分位三个子问题：leader选举、日志复制、安全性  
在raft协议下，系统分为一个个连续的term阶段，term的值是递增的，一个term阶段可能存在leader，也可能不存在leader  
- leader选举
所有节点上线后都处于follower状态，当在一定间隔内（\[150, 300\]ms随机）未收到leader心跳则认为leader下线，切换为candidate状态，将最近的term值自增，然后向其他节点发起投票，follower会投票给收到的第一个合法拉票请求，合法拉票请求的term需要大于其自身的term，且请求的log id不低于自身的term，每个follower在一个term只能投一张票  
若candidate节点被大多数节点投票，则成为leader，需要周期性的向follower发起心跳来维持其leader地位  
**由于在上一个term，所有的日志都保证在大多数节点中生效，那么新的选举成功的follower一定是日志最新的节点，因为只有这个节点才能达到大多数选票**  

- 日志复制
raft协议中，客户端只通过leader提交日志，leader收到日志后封装为log entry，按顺序发送给follower，当收到大多数回应后，在本地的复制状态机应用该日志并响应client  

- 安全性
**选举安全性:** 即任一任期内最多一个leader被选出。在一个集群中任何时刻只能有一个leader。系统中同时有多余一个leader，被称之为脑裂（brain split），这是非常严重的问题，会导致数据的覆盖丢失。    
在raft中，通过两点保证了这个属性：  
一个节点某一任期内最多只能投一票；而节点B的term必须比A的新，A才能给B投票。
只有获得多数投票的节点才会成为leader。  
**日志append only：** 首先，leader在某一term的任一位置只会创建一个log entry，且log entry是append-only。  
其次，一致性检查。leader在AppendEntries请求中会包含最新log entry的前一个log的term和index，如果follower在对应的term index找不到日志，那么就会告知leader日志不一致， 然后开始同步自己的日志。同步时，找到日志分叉点，然后将leader后续的日志复制到本地  

**日志匹配特性:**  如果两个节点上的某个log entry的log index相同且term相同，那么在该index之前的所有log entry应该都是相同的。  
Raft的日志机制提供两个保证，统称为Log Matching Property：  
不同机器的日志中如果有两个entry有相同的偏移和term号，那么它们存储相同的指令。
如果不同机器上的日志中有两个相同偏移和term号的日志，那么日志中这个entry之前的所有entry保持一致  
**Leader完备性:** 被选举人必须比自己知道的更多（比较term 、log index）
如果一个log entry在某个任期被提交（committed），那么这条日志一定会出现在所有更高term的leader的日志里面 。 选举人必须比自己知道的更多（比较term，log index）如果日志中含有不同的任期，则选最新的任期的节点； 如果最新任期一致，则选最长的日志的那个节点。
选举安全性中包含了Leader完备性

**状态机安全性:** 状态机安全性由日志的一致来保证。在算法中，一个日志被复制到多数节点才算committed， 如果一个log entry在某个任期被提交（committed），那么这条日志一定会出现在所有更高term的leader的日志里面。  

### zab

zab协议与raft协议类似，也存在leader节点，有leader接收client的写并同步给follower，zab协议过程分为两个阶段  
- 选举
- 同步
- 广播

zookeeper中实现的zab协议工作在两种模式  
- 恢复模式
当系统刚上线或者leader下线后，将进入恢复模式，选举出leader，该过程每个节点进行竞争，初始都投票给自己，当一个节点收到半数以上的竞选请求后，依次比较（echo、zid、servierid）三个值，选择最大的一个节点进行支持，以此来选出leader，然后进行事务的同步，leader先确定所有节点的最大事务id，同步到该状态，然后再将其状态同步给其他节点  
- 广播模式
当半数以上的follower跟leader同步完成后，系统进入广播模式，此时才能处理client的写事务，leader采取两阶段提交方式，先提交prepare，收到大多数节点响应后进行commit，然后响应client  

### raft与zab的区别
**参考文章：[article](https://niceaz.com/2018/11/03/raft-and-zab/#lease-read)**  
raft与zab有许多相似之处，都是multi-paxos类型协议，都有leader，raft的term、日志也跟zab的epoch、事务概念对应，但他们也存在不少的不同之处  
- zab可在一次投票确定leader，raft却未必，可能需要多个term才能完成选举  
- zab选举时根据epoch、zid、serverId三个数据确定对应值最大的leader（epoch相同比较zid，zid相同比较serverId），raft每个follower在\[150， 300\]ms的随机间隔后未收到leader信息进行竞争leader，follower会投票个接收到的第一个合法竞选者，而不是想zab中等收到大多数竞选票后再投（这也是zab一次能选出leader的原因）  

**通常数paxos、raft、zab实现了强一致性，可认为是线性一致性，即一个client读取多次，得到的状态是递增的，不会读取到比之前更旧的数据**  



## 分布式系统一致性

**参考文章：[article](https://blog.csdn.net/u010180738/article/details/123674843)**  