---
title: BladeKV
auther: ahou
layout: post
date: 2022/08/30
---


## 项目背景

分布式KV引擎的开发，该项目目标是基于RocksDB构建高性能、高可用的KV存储引擎，支持大量数据的快速离线导入，在性能持平redis的同时保证成本上的优势，该项目从今年2月份开始开发，现在已经接入了业务的生产数据，服务的峰值访问约60W QPS，性能方面平均耗时1ms，P99耗时小于5ms

我在该项目中主要负责客户端和存储端的设计与开发，另外还包括集群的维护，对接业务，保障业务顺利接入等工作


引擎特点：

高性能，高可用，性能持平redis（针对业务场景对RocksDB的调优）

分布式、高可用（分片、多副本）

支持海量数据存储，快速的离线导入能力

Redis基于内存，难以支持海量数据或者成本过高



分组导入，启用会有短暂的不一致问题，新开发扩缩容版本，可在导入后同时启用



高性能原因

导入：

离线导入，以partition为并发度，可快速生成sst文件，集群仅需要拉取文件进行rocksdb实例构建，占导入时间的比例很低，一般2T数据的文件生成和导入大概2小时，但副本导入在10分钟以内集群导入仅20分钟左右

查询：

客户端: 确定分区后，直连存储节点，没有中间节点，初始化时提前构建查询代理并预热连接，避免启动时毛刺

存储端: 

1、通过离线导入ingest构建的RocksDB实例理论上性能优正常put写入的实例，离线导入经过全局排序，sst都在lsm树的最底层，查询时最多只需要访问一个sst文件，而多层的lsm树一般需要访问多个sst；

2、预热，实现线上流量和本地文件两种预热方式，在启动和版本切换时提前预热db cache和page cache

3、根据业务数据和查询特点对RocksDB做调优，写入时的block size、重启点间隔、压缩算法，读取时的block cache、cache的share bit、filter和index的缓存过期策略

4、上报预聚合





排查解决问题：

毛刺，客户端容器发生老年代gc引发毛刺

间歇性出现超时，排查gc无问题，容器资源占用也正常，检查网络情况发现宿主机偶尔会出现异常流量，造成服务的网络耗时增加



未来的方向：

支持写入

打通自动化建表导入流程

一定程度的自动化运维，故障节点自动替换，集群版本发布流程



##  组内所做的引擎哪些方面的工作


rocksDB调优的参数

写入：

- block size：

当读取的key比较随机且属性较少时，可能将block size设的小一些，这样同样的cache可以存储更多的block，随机读取效果会好一些，别不我们一个表的场景下，每次仅读取一个field，block size设置为4K 

较大的block size适合key比较大、每次读取涉及多个value的情况，这样每次可将需要的数据加载到缓存里

- restart interval：

较大时查找效率会降低，但前缀压缩效果会更好，磁盘占用少

较小时查询效率会高（二分确定间隔后顺序遍历的值更少），磁盘占用高

复杂读，log n确定区间，遍历x确定值，总的是xlogn

- 压缩算法：
采用解压更快的 LZ4_COMPRESSION

读取：
- block cache：

根据机器预计存储的数据量和可用的内存，尽量分配合适的cache大小

- cache的share bit：

LRU cache中包含多个hashMap，share相当于cache的并发度

- 缓存data、filter、index block的策略

针对线上查询命中率进行优化，空属性比例接近50%，索引和过滤器可以产生比较好的收益

index和filter是的top层是常驻内存的

index和filter缓存配置了更高的优先级


